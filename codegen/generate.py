# Copyright (C) 2023 - 2026 ANSYS, Inc. and/or its affiliates.
# SPDX-License-Identifier: MIT
#
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""This script generates the keyword classes for the LSPP.  It uses the kwd.json file to get the keyword"""
import argparse
import copy
import logging
import os
import typing

from beartype.claw import beartype_package

beartype_package("keyword_generation")  # Enable beartype for keyword_generation package

from jinja2 import Environment, FileSystemLoader
import keyword_generation.data_model as data_model
from keyword_generation.generators import generate_class, generate_entrypoints
from keyword_generation.generators.template_context import DocTemplateContext
from keyword_generation.utils import get_this_folder, handle_single_word_keyword
from keyword_generation.utils.keyword_utils import KeywordNames, filter_keywords_by_domain
from output_manager import OutputManager

logger = logging.getLogger(__name__)

SKIPPED_KEYWORDS = set(
    [
        # defined manually because of the variable length text card
        "DEFINE_FUNCTION",
        # element_solid (10 nodes format) - merging the element solids
        "ELEMENT_SOLID (ten nodes format)",
        "ELEMENT_SOLID",
        "ELEMENT_SOLID_ORTHO (ten nodes format)",
        "ELEMENT_SOLID_ORTHO",
        # issue #184 - this is not documented in the manual
        # "CONTROL_TIMESTEP",CONTROL_TIMESTEP is in the kwd.json now and should be generated issue #629
    ]
)


def get_loader():
    template_folder = get_this_folder() / "templates"
    return FileSystemLoader(str(template_folder.resolve()))


def is_wildcard_excluded(keyword, wildcard):
    exclusions = wildcard.get("exclusions", [])
    if len(exclusions) > 0:
        if wildcard["type"] != "prefix":
            raise Exception("Exclusions not only allowed for prefix wildcard")
    for exclusion in exclusions:
        if keyword.startswith(exclusion):
            return True
    return False


def match_wildcard_pattern(keyword, pattern, wildcard_type):
    if wildcard_type == "prefix":
        return keyword.startswith(pattern)
    elif wildcard_type == "exact":
        return keyword == pattern
    raise Exception("Unexpected wildcard type")


def match_wildcard(keyword, wildcard):
    if is_wildcard_excluded(keyword, wildcard):
        logger.debug(f"Keyword {keyword} excluded from wildcard {wildcard.get('patterns', [])}")
        return False
    wildcard_type = wildcard["type"]
    for pattern in wildcard["patterns"]:
        if match_wildcard_pattern(keyword, pattern, wildcard_type):
            logger.debug(f"Keyword {keyword} matched wildcard pattern {pattern} (type: {wildcard_type})")
            return True
    return False


def skip_generate_keyword_class(keyword: str) -> bool:
    global SKIPPED_KEYWORDS
    if keyword in SKIPPED_KEYWORDS:
        logger.debug(f"Skipping keyword: {keyword} (in SKIPPED_KEYWORDS)")
        return True
    return False


def get_undefined_alias_keywords(
    keywords_list: typing.List[typing.Dict], subset_domains: typing.Optional[typing.List[str]] = None
) -> typing.List[typing.Dict]:

    config = data_model.get_config()
    undefined_aliases: typing.List[typing.Dict] = []
    for alias, kwd in config.get_aliases().items():
        if alias not in [kwd["name"] for kwd in keywords_list]:
            # Filter by subset domains if specified
            if subset_domains and not filter_keywords_by_domain([alias], subset_domains):
                continue

            alias_names = KeywordNames.from_keyword(alias)
            base_names = KeywordNames.from_keyword(kwd)
            alias_kwd = {
                "name": alias,
                "is_autogenerated": True,
                "filename": base_names.filename,
                "classname": alias_names.classname,
                "title": alias,
            }
            undefined_aliases.append(alias_kwd)
    return undefined_aliases


def merge_options(keyword_options: typing.Dict, generation_settings: typing.Dict) -> None:
    generation_settings = copy.deepcopy(generation_settings)
    logger.debug(f"Merging generation settings: {list(generation_settings.keys())}")
    if keyword_options == {}:
        keyword_options.update({"generation-options": generation_settings})
    else:
        generation_options: typing.Dict = keyword_options.get("generation-options", {})
        if generation_options == {}:
            generation_options.update(generation_settings)
        else:
            generation_option_keys = set(generation_options.keys())
            generation_setting_keys = set(generation_settings.keys())
            intersecting_keys = generation_option_keys & generation_setting_keys
            num_new_keys = len(generation_setting_keys - generation_option_keys)
            logger.debug(f"Merging {len(intersecting_keys)} intersecting keys, adding {num_new_keys} new keys")
            for intersecting_key in intersecting_keys:
                generation_optinon: typing.List = generation_options[intersecting_key]
                generation_optinon.extend(generation_settings[intersecting_key])
            difference_keys = generation_setting_keys - generation_option_keys
            for difference_key in difference_keys:
                generation_options[difference_key] = generation_settings[difference_key]


def handle_wildcards(keyword_options: typing.Dict, keyword: str) -> None:
    if skip_generate_keyword_class(keyword):
        return
    if "wildcards_handled" in keyword_options.keys():
        return
    config = data_model.get_config()
    for wildcard in config.manifest["WILDCARDS"]:
        if match_wildcard(keyword, wildcard):
            merge_options(keyword_options, wildcard["generation-options"])
            # Also merge labels from wildcard if present
            if "labels" in wildcard:
                kwd_labels = keyword_options.setdefault("labels", {})
                for label, index in wildcard["labels"].items():
                    if label not in kwd_labels:
                        kwd_labels[label] = index
    keyword_options["wildcards_handled"] = True


def get_keyword_options(keyword: str, wildcards: bool = True) -> typing.Dict:
    """Returns the generation options of the given keyword from the manifest.  If apply_wildcards is True,
    this will return the generataion options of the keyword merged with the generation options of the
    wildard that matches this keyword, if any."""
    config = data_model.get_config()
    keyword_options = config.manifest.get(keyword, {})
    if wildcards:
        handle_wildcards(keyword_options, keyword)
    return keyword_options


def get_keyword_item(keyword: str) -> typing.Dict[str, typing.Any]:
    keyword_options = get_keyword_options(keyword)
    names = KeywordNames.from_keyword(keyword)
    classname = keyword_options.get("classname", names.classname)
    aliased_by = data_model.get_aliased_by(keyword)
    if aliased_by:
        alias_names = KeywordNames.from_keyword(aliased_by)
        filename = alias_names.filename
        logger.debug(f"Keyword {keyword} is aliased by {aliased_by}, using filename: {filename}")
    else:
        filename = names.filename
    keyword_item = {
        "name": keyword,
        "title": handle_single_word_keyword(keyword),
        "classname": classname,
        "filename": filename,
        "is_autogenerated": not skip_generate_keyword_class(keyword),
    }
    logger.debug(f"Created keyword item for {keyword}: classname={classname}, filename={filename}")
    return keyword_item


def get_generations(keyword: str) -> typing.List[typing.Tuple]:
    keyword_options = get_keyword_options(keyword)
    if keyword_options.get("type") != "multiple":
        return [(keyword, keyword_options)]
    generations = keyword_options.get("generations", [])
    result = [(gen["keyword"], gen) for gen in generations]
    return result


def add_aliases(kwd_list: typing.List[str]) -> None:
    logger.debug(f"Processing aliases for {len(kwd_list)} keywords")
    alias_count = 0
    for keyword in kwd_list:
        keyword_options = get_keyword_options(keyword, False)
        if "alias" in keyword_options:
            alias = keyword_options["alias"]
            data_model.add_alias(keyword, alias)
            logger.debug(f"Added alias: {keyword} -> {alias}")
            alias_count += 1
    logger.debug(f"Total aliases added: {alias_count}")


def generate_autodoc_file(autodoc_output_path, all_keywords, env):
    """Generates the autodoc rst files for all keywords, organized by category."""
    # Organize keywords by category
    categories = {}
    for item in all_keywords:
        keyword = item["name"]
        if skip_generate_keyword_class(keyword):
            continue
        if data_model.is_aliased(keyword):
            continue
        names = KeywordNames.from_keyword(keyword)
        classname = item["options"].get("classname", names.classname)

        # Extract category from filename (e.g., "airbag.airbag_adiabatic_gas_model" -> "airbag")
        filename = names.filename
        if "." in filename:
            category = filename.split(".")[0]
        else:
            category = filename.split("_")[0] if "_" in filename else "other"

        if category not in categories:
            categories[category] = []
        categories[category].append((classname, filename))

    logger.info(f"Organized {len(all_keywords)} keywords into {len(categories)} categories")

    # Generate a separate RST file for each category
    category_template = env.get_template("autodoc_category.jinja")
    output_manager = OutputManager(os.path.dirname(autodoc_output_path))

    for category, entries in sorted(categories.items()):
        category_title = category.replace("_", " ").capitalize() + " keywords"
        # Use structured context for category rendering
        # Note: category_title and entries are still passed directly for template compatibility
        category_rst = category_template.render(
            category=category, category_title=category_title, entries=sorted(entries)
        )
        category_filename = f"{category}.rst"
        output_manager.write_autodoc_file(autodoc_output_path, category_filename, category_rst)
        logger.debug(f"Generated category file: {category_filename} with {len(entries)} entries")

    # Generate index file with toctree of all categories
    index_template = env.get_template("autodoc_index.jinja")
    context = DocTemplateContext(categories=sorted(categories.keys()))
    index_rst = index_template.render(**context.to_dict())
    output_manager.write_autodoc_file(autodoc_output_path, "index.rst", index_rst)
    logger.info(f"Generated index.rst with {len(categories)} category links")


def get_keywords_to_generate(
    kwd_name: typing.Optional[str] = None, subset_domains: typing.Optional[typing.List[str]] = None
) -> typing.List[typing.Dict]:
    """Get keywords to generate. If a kwd name is not none, only generate
    it and its generations. If subset_domains is provided, only generate keywords
    from those domains (e.g., ['boundary', 'contact', 'control'])."""
    config = data_model.get_config()
    keywords = []
    kwd_list = config.keyword_data.get_keywords_list()

    # first get all aliases
    add_aliases(kwd_list)

    # then get keywords to generate
    for keyword in kwd_list:
        if kwd_name != None and keyword != kwd_name:
            continue

        # Filter by subset domains if specified
        if subset_domains and not filter_keywords_by_domain([keyword], subset_domains):
            continue

        for keyword, keyword_options in get_generations(keyword):
            item = get_keyword_item(keyword)
            item["options"] = keyword_options
            keywords.append(item)

    return keywords


def generate_classes(
    lib_path: str,
    kwd_name: typing.Optional[str] = None,
    autodoc_output_path: str = "",
    subset_domains: typing.Optional[typing.List[str]] = None,
) -> None:
    """Generates the keyword classes, importer, and type-mapper
    if kwd_name is not None, this only generates that particular keyword class
    if subset_domains is not None, only generates keywords from those domains
    """
    logger.debug(
        f"Starting class generation with lib_path={lib_path}, kwd_name={kwd_name}, subset_domains={subset_domains}"
    )
    if subset_domains:
        logger.info(f"Subset mode: generating only domains {subset_domains}")
    autodoc_entries = []
    env = Environment(loader=get_loader(), trim_blocks=True, lstrip_blocks=True)
    output_manager = OutputManager(lib_path)
    # Generate only requested keyword(s)
    keywords_list = get_keywords_to_generate(kwd_name, subset_domains)
    logger.info(f"Generating {len(keywords_list)} keyword classes")
    generated_count = 0
    skipped_count = 0
    for item in keywords_list:
        name = item["name"]
        if skip_generate_keyword_class(name):
            skipped_count += 1
            continue
        if data_model.is_aliased(name):
            logger.debug(f"Skipping aliased keyword: {name}")
            skipped_count += 1
            continue
        # Use output_manager for file writing in generate_class
        logger.debug(f"Generating class for keyword: {name}")
        classname, filename = generate_class(env, output_manager, item)
        autodoc_entries.append((classname, filename))
        generated_count += 1
    logger.info(f"Generated {generated_count} classes, skipped {skipped_count}")

    # Always rewrite autodoc for all keywords
    if autodoc_output_path and not kwd_name:
        all_keywords = get_keywords_to_generate(subset_domains=subset_domains)
        generate_autodoc_file(autodoc_output_path, all_keywords, env)
    keywords_list.extend(get_undefined_alias_keywords(keywords_list, subset_domains))
    if kwd_name == None:
        generate_entrypoints(env, output_manager, keywords_list)


def clean(output):
    """Removes the files that were generated by this system"""
    output_manager = OutputManager(output)
    output_manager.clean()
    logger.info("Cleaning attempted (see output folder)")


def load_inputs(this_folder, args):
    return data_model.load(str(this_folder), args.kwd_file, args.manifest, args.additional_cards)


def run_codegen(args):
    logger.debug(
        f"Running codegen with args: output={args.output}, "
        f"clean={args.clean}, keyword={args.keyword}, autodoc_only={args.autodoc_only}"
    )
    output = args.output
    this_folder = get_this_folder()
    autodoc_path = args.autodoc_path
    if not autodoc_path:
        autodoc_path = this_folder.parent / "doc" / "source" / "_autosummary"
    if not os.path.exists(autodoc_path):
        os.makedirs(autodoc_path)
        logger.debug(f"Created autodoc directory: {autodoc_path}")
    autodoc_path = str(autodoc_path.resolve())
    if args.output == "":
        output = this_folder.parent / "src" / "ansys" / "dyna" / "core" / "keywords" / "keyword_classes"
        output = str(output.resolve())
        logger.debug(f"Using default output path: {output}")
    else:
        output = args.output
        logger.debug(f"Using custom output path: {output}")
    if args.clean:
        logger.info("Clean mode requested")
        clean(output)
        return
    load_inputs(this_folder, args)

    # Handle subset domains
    subset_domains = None
    if args.subset:
        subset_domains = [d.strip() for d in args.subset.split(",")]
        logger.info(f"Subset mode enabled: generating only {subset_domains} domains")

    # Handle autodoc-only mode
    if args.autodoc_only:
        logger.info("Generating autodoc files only")
        env = Environment(loader=get_loader(), trim_blocks=True, lstrip_blocks=True)
        all_keywords = get_keywords_to_generate(subset_domains=subset_domains)
        generate_autodoc_file(autodoc_path, all_keywords, env)
        logger.info("Autodoc generation complete")
        return

    # Handle subset domains
    subset_domains = None
    if args.subset:
        subset_domains = [d.strip() for d in args.subset.split(",")]
        logger.info(f"Subset mode enabled: generating only {subset_domains} domains")

    if args.keyword == "":
        kwd = None
        logger.info(
            "Generating code for all keywords" if not subset_domains else f"Generating subset: {subset_domains}"
        )
        generate_classes(output, autodoc_output_path=autodoc_path, subset_domains=subset_domains)
    else:
        kwd = args.keyword
        logger.info(f"Generating code for {kwd}")
        generate_classes(output, kwd, autodoc_output_path=autodoc_path, subset_domains=subset_domains)


def parse_args():
    parser = argparse.ArgumentParser(description="Run pydyna codegen")
    parser.add_argument(
        "--output",
        "-o",
        default="",
        help="Output folder.",
        # help="Output folder. Defaults to the location of generated code in pydyna."
    )
    parser.add_argument(
        "--clean", "-c", action="store_true", help="Wipes away the generated code in the output folder."
    )
    parser.add_argument(
        "--keyword",
        "-k",
        default="",
        help="optional - keyword for which to generate.  If not set, all keywords are generated.",
    )
    parser.add_argument(
        "--autodoc-only",
        "-a",
        action="store_true",
        help="Generate only the autodoc RST files without generating keyword classes.",
    )
    parser.add_argument(
        "--manifest",
        "-m",
        default="",
        help="Path to manifest, defaults to manifest.json in the same folder as this file.",
    )
    parser.add_argument(
        "--kwd-file", default="", help="Path to keyword file, defaults to kwd.json in the same folder as this file."
    )
    parser.add_argument(
        "--additional-cards",
        default="",
        help="Path to additional cards file, defaults to additional-cards.json in the same folder as this file.",
    )
    parser.add_argument(
        "autodoc_path",
        default="",
        nargs="?",
        help="Path to the autodoc output folder. Defaults to doc/source/_autosummary.",
    )
    parser.add_argument(
        "--log-level",
        "-l",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level. Defaults to INFO.",
    )
    parser.add_argument(
        "--subset",
        "-s",
        default="",
        help="Generate only a subset of keyword domains (comma-delimited list, e.g., 'boundary,contact,control')."
        "Useful for fast iteration during optimization work.",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    run_codegen(args)
