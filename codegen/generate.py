# Copyright (C) 2023 - 2025 ANSYS, Inc. and/or its affiliates.
# SPDX-License-Identifier: MIT
#
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""This script generates the keyword classes for the LSPP.  It uses the kwd.json file to get the keyword"""
import argparse
import copy
import logging
import os
import shutil
import typing

from jinja2 import Environment, FileSystemLoader
import keyword_generation.data_model as data_model
from keyword_generation.generators import generate_class, generate_entrypoints
from keyword_generation.utils import fix_keyword, get_classname, get_this_folder, handle_single_word_keyword

SKIPPED_KEYWORDS = set(
    [
        # defined manually because of the variable length text card
        "DEFINE_FUNCTION",
        # element_solid (10 nodes format) - merging the element solids
        "ELEMENT_SOLID (ten nodes format)",
        "ELEMENT_SOLID",
        "ELEMENT_SOLID_ORTHO (ten nodes format)",
        "ELEMENT_SOLID_ORTHO",
        # issue #184 - this is not documented in the manual
        # "CONTROL_TIMESTEP",CONTROL_TIMESTEP is in the kwd.json now and should be generated issue #629
    ]
)


def get_loader():
    template_folder = get_this_folder() / "templates"
    return FileSystemLoader(str(template_folder.resolve()))


def match_wildcard(keyword, wildcard):
    assert wildcard["type"] == "prefix"
    exclusions = wildcard.get("exclusions", [])
    for exclusion in exclusions:
        if keyword.startswith(exclusion):
            return False
    for pattern in wildcard["patterns"]:
        if keyword.startswith(f"{pattern}"):
            return True
    return False


def skip_generate_keyword_class(keyword: str) -> bool:
    global SKIPPED_KEYWORDS
    if keyword in SKIPPED_KEYWORDS:
        return True
    return False


def get_undefined_alias_keywords(keywords_list: typing.List[typing.Dict]) -> typing.List[typing.Dict]:
    undefined_aliases: typing.List[typing.Dict] = []
    for alias, kwd in data_model.ALIAS_TO_KWD.items():
        if alias not in [kwd["name"] for kwd in keywords_list]:
            fixed_keyword = fix_keyword(alias).lower()
            classname = get_classname(fixed_keyword)
            fixed_base_keyword = fix_keyword(kwd).lower()
            alias_kwd = {
                "is_autogenerated": True,
                "filename": fixed_base_keyword,
                "classname": classname,
                "title": alias,
            }
            undefined_aliases.append(alias_kwd)
    return undefined_aliases


def merge_options(keyword_options: typing.Dict, generation_settings: typing.Dict) -> None:
    generation_settings = copy.deepcopy(generation_settings)
    if keyword_options == {}:
        keyword_options.update({"generation-options": generation_settings})
    else:
        generation_options: typing.Dict = keyword_options.get("generation-options", {})
        if generation_options == {}:
            generation_options.update(generation_settings)
        else:
            generation_option_keys = set(generation_options.keys())
            generation_setting_keys = set(generation_settings.keys())
            intersecting_keys = generation_option_keys & generation_setting_keys
            for intersecting_key in intersecting_keys:
                generation_optinon: typing.List = generation_options[intersecting_key]
                generation_optinon.extend(generation_settings[intersecting_key])
            difference_keys = generation_setting_keys - generation_option_keys
            for difference_key in difference_keys:
                generation_options[difference_key] = generation_settings[difference_key]


def handle_wildcards(keyword_options: typing.Dict, keyword: str) -> None:
    if skip_generate_keyword_class(keyword):
        return
    if "wildcards_handled" in keyword_options.keys():
        return
    for wildcard in data_model.MANIFEST["WILDCARDS"]:
        if match_wildcard(keyword, wildcard):
            merge_options(keyword_options, wildcard["generation-options"])
    keyword_options["wildcards_handled"] = True


def get_keyword_options(keyword: str, wildcards: bool = True) -> typing.Dict:
    """Returns the generation options of the given keyword from the manifest.  If apply_wildcards is True,
    this will return the generataion options of the keyword merged with the generation options of the
    wildard that matches this keyword, if any."""
    keyword_options = data_model.MANIFEST.get(keyword, {})
    if wildcards:
        handle_wildcards(keyword_options, keyword)
    return keyword_options


def get_keyword_item(keyword: str) -> None:
    keyword_options = get_keyword_options(keyword)
    fixed_keyword = fix_keyword(keyword).lower()
    classname = keyword_options.get("classname", get_classname(fixed_keyword))
    aliased_by = data_model.get_aliased_by(keyword)
    if aliased_by:
        filename = fix_keyword(aliased_by).lower()
    else:
        filename = fixed_keyword
    keyword_item = {
        "name": keyword,
        "title": handle_single_word_keyword(keyword),
        "classname": classname,
        "filename": filename,
        "is_autogenerated": not skip_generate_keyword_class(keyword),
    }
    return keyword_item


def get_generations(keyword: str) -> typing.List[typing.Tuple]:
    keyword_options = get_keyword_options(keyword)
    if keyword_options.get("type") != "multiple":
        return [(keyword, keyword_options)]
    generations = keyword_options.get("generations")
    result = [(gen["keyword"], gen) for gen in generations]
    return result


def add_aliases(kwd_list: typing.List[str]) -> None:
    for keyword in kwd_list:
        keyword_options = get_keyword_options(keyword, False)
        if "alias" in keyword_options:
            data_model.add_alias(keyword, keyword_options["alias"])


def get_keywords_to_generate(kwd_name: typing.Optional[str] = None) -> typing.List[typing.Dict]:
    """Get keywords to generate. If a kwd name is not none, only generate
    it and its generations."""
    keywords = []
    kwd_list = data_model.KWDM_INSTANCE.get_keywords_list()

    # first get all aliases
    add_aliases(kwd_list)

    # then get keywords to generate
    for keyword in kwd_list:
        if kwd_name != None and keyword != kwd_name:
            continue
        for keyword, keyword_options in get_generations(keyword):
            item = get_keyword_item(keyword)
            item["options"] = keyword_options
            keywords.append(item)

    return keywords

def generate_index_rst(autodoc_path: str, title: str = "Keyword Classes") -> None:
    # List all .rst files (excluding index.rst itself)
    rst_files = sorted([
        f for f in os.listdir(autodoc_path)
        if f.endswith(".rst") and f != "index.rst"
    ])
    # Title and underline
    index_content = f"{title}\n{'=' * len(title)}\n\n"
    index_content += ".. toctree::\n"
    index_content += "   :maxdepth: 1\n"
    index_content += "   :caption: Contents:\n\n"
    # Add each file (without extension)
    for rst_file in rst_files:
        filename = os.path.splitext(rst_file)[0]
        index_content += f"   {filename}\n"
    # Write index.rst
    index_path = os.path.join(autodoc_path, "index.rst")
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(index_content)

def generate_classes(lib_path: str, kwd_name: typing.Optional[str] = None,
                     autodoc_output_path: str = "") -> None:
    """Generates the keyword classes, importer, and type-mapper
    if kwd_name is not None, this only generates that particular keyword class
    """
    env = Environment(loader=get_loader(), trim_blocks=True, lstrip_blocks=True)
    if not os.path.exists(os.path.join(lib_path, "auto")):
        os.mkdir(os.path.join(lib_path, "auto"))
    keywords_list = get_keywords_to_generate(kwd_name)
    for item in keywords_list:
        name = item["name"]
        if skip_generate_keyword_class(name):
            continue
        if data_model.is_aliased(name):
            continue
    generate_class(env, lib_path, item, autodoc_output_path)
    keywords_list.extend(get_undefined_alias_keywords(keywords_list))
    if kwd_name == None:
        generate_entrypoints(env, lib_path, keywords_list)


def clean(output):
    """Removes the files that were be generated by this system"""
    try:
        os.remove(os.path.join(output, "auto_keywords.py"))
        os.remove(os.path.join(output, "type_mapping.py"))
        shutil.rmtree(os.path.join(output, "auto"))
        print("Cleaning successful")
    except FileNotFoundError:
        print("Cleaning failed, files not found. Might be cleaned already")


def load_inputs(this_folder, args):
    return data_model.load(this_folder, args.kwd_file, args.manifest, args.additional_cards)


def run_codegen(args):
    output = args.output
    this_folder = get_this_folder()
    autodoc_path = args.autodoc_path
    if not autodoc_path:
        autodoc_path = this_folder.parent / "doc" / "source" / "keywords"
    if not os.path.exists(autodoc_path):
        os.makedirs(autodoc_path)
    autodoc_path = str(autodoc_path.resolve())
    if args.output == "":
        output = this_folder.parent / "src" / "ansys" / "dyna" / "core" / "keywords" / "keyword_classes"
        output = str(output.resolve())
    else:
        output = args.output
    if args.clean:
        clean(output)
        return
    load_inputs(this_folder, args)
    if args.keyword == "":
        kwd = None
        print(f"Generating code for all keywords")
        generate_classes(output, autodoc_output_path=autodoc_path)
    else:
        kwd = args.keyword
        print(f"Generating code for {kwd}")
        generate_classes(output, autodoc_path, kwd)
    generate_index_rst(autodoc_path)

def parse_args():
    parser = argparse.ArgumentParser(description="Run pydyna codegen")
    parser.add_argument(
        "--output",
        "-o",
        default="",
        help="Output folder.",
        # help="Output folder. Defaults to the location of generated code in pydyna."
    )
    parser.add_argument(
        "--clean", "-c", action="store_true", help="Wipes away the generated code in the output folder."
    )
    parser.add_argument(
        "--keyword",
        "-k",
        default="",
        help="optional - keyword for which to generate.  If not set, all keywords are generated.",
    )
    parser.add_argument(
        "--manifest",
        "-m",
        default="",
        help="Path to manifest, defaults to manifest.json in the same folder as this file.",
    )
    parser.add_argument(
        "--kwd-file", default="", help="Path to keyword file, defaults to kwd.json in the same folder as this file."
    )
    parser.add_argument(
        "--additional-cards",
        default="",
        help="Path to additional cards file, defaults to additional-cards.json in the same folder as this file.",
    )
    parser.add_argument(
        "autodoc_path",
        default="",
        nargs="?",
        help="Path to the autodoc output folder. Defaults to doc/source/keywords.",
    )
    return parser.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.WARNING)

    args = parse_args()
    run_codegen(args)
